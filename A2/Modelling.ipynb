{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e875141",
   "metadata": {},
   "source": [
    "# Models, Evaluation, TrainTestSplit implementations\n",
    "\n",
    "#### if you wanna run experiment scroll down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3622df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import csv\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# user based collaborative filtering\n",
    "# if you want to run CF to create submission file uncomment write lines in fit function and comment output_list\n",
    "# lines so you dont store 2gb in your ram memory\n",
    "class CollaborativeFiltering:\n",
    "    \n",
    "    # we need more search features\n",
    "    # we can remove all hotel features\n",
    "    def __init__(self):\n",
    "        # self.search_features = [\"srch_adults_count\", \"srch_saturday_night_bool\", \"site_id\", \"visitor_location_country_id\", \"srch_length_of_stay\", \"srch_room_count\"]\n",
    "        self.search_features = ['promotion_flag', 'srch_length_of_stay','srch_booking_window', 'srch_saturday_night_bool',\\\n",
    "                                'srch_query_affinity_score', 'orig_destination_distance', 'user_hotel_price', 'desirability',\\\n",
    "                                'd_p_ratio', 'people_per_room', 'overall_advantage','site_id', 'visitor_location_country_id']\n",
    "        \n",
    "    def cos_sim(self, user_data, test_data):\n",
    "        categorical = [\"site_id\", \"visitor_location_country_id\", \"srch_saturday_night_bool\", \"promotion_flag\"]\n",
    "        user_categorical, user_numeric = user_data[categorical] , user_data.drop(categorical)\n",
    "        test_categorical, test_numeric = test_data[categorical], test_data.drop(categorical)\n",
    "        \n",
    "        # for numeric calc cos_sim\n",
    "        a = user_numeric.to_numpy()\n",
    "        b = test_numeric.to_numpy()\n",
    "        \n",
    "        cos_sim_score = dot(a, b)/(norm(a)*norm(b))\n",
    "        \n",
    "        if(math.isnan(cos_sim_score)):\n",
    "            return -1\n",
    "        \n",
    "        # for cateforical_data increase cos_sim_score if it matches\n",
    "        for idx, col in enumerate(user_categorical):\n",
    "            if user_categorical[idx] == test_categorical[idx]:\n",
    "                cos_sim_score += 0.2\n",
    "    \n",
    "        return cos_sim_score\n",
    "    \n",
    "    def get_cosine_similarities(self, train_data, user_data):\n",
    "        cosine_scores = []\n",
    "        for idx, t_data in train_data.iterrows():\n",
    "            cosine_scores.append((self.cos_sim(user_data, t_data) , idx))\n",
    "            \n",
    "        return sorted(cosine_scores, reverse=True)\n",
    "    \n",
    "    def find_n_nearest_searches(self, user_data, n, hotel_id):\n",
    "        # take only those users who have data for this hotel\n",
    "        train_data = self.train_data.loc[self.train_data[\"prop_id\"] == hotel_id]\n",
    "        train_data = train_data[self.search_features]\n",
    "        # get n best rows (with highest cosine similiarity)\n",
    "        similiarities = self.get_cosine_similarities(train_data, user_data)[:n]\n",
    "        # print(f\"sims: {similiarities}\")\n",
    "        idx = [i[1] for i in similiarities]\n",
    "        # print(f\"indeces {idx}\")\n",
    "        return self.train_data.loc[idx]\n",
    "        \n",
    "    def score_function(self, n_nearest_searches):\n",
    "        if n_nearest_searches.empty:\n",
    "            return -10\n",
    "        score = 0\n",
    "        b_w = 100 # booking weight\n",
    "        for _, x in n_nearest_searches.iterrows():\n",
    "            score += b_w * x[\"booking_bool\"] + x[\"click_bool\"] - x[\"position\"]\n",
    "        # return avg score \n",
    "        return score/len(n_nearest_searches)\n",
    "    \n",
    "    def fit(self, train_data, test_data, n=5):\n",
    "        self.train_data = train_data\n",
    "        output_list = []\n",
    "        # for every user search\n",
    "    # with open('cf.csv', 'w') as f:\n",
    "        # writer = csv.writer(f)\n",
    "        # writer.writerow([\"srch_id\", \"prop_id\"])\n",
    "        for srch_id in test_data[\"srch_id\"].unique():\n",
    "            user_searches = test_data.loc[test_data[\"srch_id\"] == srch_id]\n",
    "            hotel_scores = []\n",
    "            user_data = user_searches[self.search_features].iloc[0]\n",
    "            for idx, search in user_searches.iterrows():\n",
    "                # find n nearest users user_data + search_specific_data\n",
    "                hotel_id = search[\"prop_id\"]\n",
    "                # n nearest searches that scored a given hotel\n",
    "                n_nearest_searches = self.find_n_nearest_searches(user_data, n, hotel_id)\n",
    "\n",
    "                score = self.score_function(n_nearest_searches)\n",
    "                hotel_scores.append((score, hotel_id))\n",
    "\n",
    "            # ordered list of hotel_ids w.r.t. scores\n",
    "            hotel_scores = sorted(hotel_scores, reverse=True)\n",
    "            # only save hotel ids\n",
    "            hotel_scores = [[srch_id, int(h[1])] for h in hotel_scores]\n",
    "            # writer.writerows(hotel_scores)\n",
    "            output_list += hotel_scores\n",
    "\n",
    "        return output_list\n",
    "\n",
    "# train_data = pd.read_csv(\"/Users/kuba/VU/DMT/data-mining-techniques/A2/100k_train_data.csv\")\n",
    "# test_data = pd.read_csv(\"/Users/kuba/VU/DMT/data-mining-techniques/A2/processed_test_data.csv\")\n",
    "# cf = CollaborativeFiltering()\n",
    "# out = cf.fit(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "37625113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "# baseline random\n",
    "def random_ordering(test_data):\n",
    "    with open('sub_random.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"srch_id\", \"prop_id\"])\n",
    "        for srch_id in test_data[\"srch_id\"].unique():\n",
    "            search_list = []\n",
    "            user_searches = test_data.loc[test_data[\"srch_id\"] == srch_id]\n",
    "            for idx, search in user_searches.iterrows():\n",
    "                hotel_id = search[\"prop_id\"]\n",
    "                search_list.append([srch_id, int(hotel_id)])\n",
    "\n",
    "            random.shuffle(search_list)\n",
    "            writer.writerows(search_list)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61c83ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate(predicted_ranking, actual_ranking, n=5):\n",
    "    \n",
    "    idcg = 5\n",
    "    for i in range(2, n + 1):\n",
    "        idcg += 1/math.log2(i + 1)\n",
    "        \n",
    "    i = 1\n",
    "    prev = predicted_ranking[0][0] # srch_id\n",
    "    all_ndcg = []\n",
    "    dcg = 0\n",
    "    \n",
    "    for row in predicted_ranking:\n",
    "    \n",
    "        if row[0] != prev:\n",
    "            i = 1\n",
    "            dcg = 0\n",
    "            \n",
    "        # take only first n values per search\n",
    "        if i < n + 1:\n",
    "            hotel_id = row[1]\n",
    "            # check relevance score based on if hotel was booked\n",
    "            relevance_score = 0\n",
    "            if actual_ranking[(actual_ranking['srch_id'] == row[0]) & (actual_ranking['prop_id'] == hotel_id)]['booking_bool'].any() == 1:\n",
    "                relevance_score = 5\n",
    "                # chceck relevance score based on if hotel was clicked\n",
    "            elif actual_ranking[(actual_ranking['srch_id'] == row[0]) & (actual_ranking['prop_id'] == hotel_id)]['click_bool'].any() == 1:\n",
    "                relevance_score = 1\n",
    "            # print(f\"{i}_{row[0]}_{hotel_id}: {relevance_score}\")\n",
    "            # calculate dcg iteratively per value\n",
    "            dcg += relevance_score/math.log2(i+1)\n",
    "\n",
    "        # if last partial score for dcg added then normalize and add to all ndcg list\n",
    "        if i == n:\n",
    "            # normalize\n",
    "            ndcg = dcg/idcg\n",
    "            all_ndcg.append(ndcg)\n",
    "            \n",
    "        prev = row[0]\n",
    "        i+= 1\n",
    "\n",
    "    return sum(all_ndcg)/len(all_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ec1aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_test_split(dat, n=50):\n",
    "    random.seed(1)\n",
    "    train, test = get_train_test(dat, n)\n",
    "    features_to_remove = ['click_bool','position','booking_bool', 'gross_bookings_usd']\n",
    "    test_data_sorted = test.sort_values(['srch_id','booking_bool', 'click_bool','position'], ascending = [True, False, False, True])\n",
    "    test_data_sorted.reset_index(drop=True, inplace=True)\n",
    "    # correct_order just needed for debugging\n",
    "    # correct_order = test_data_sorted[['srch_id', 'prop_id']]\n",
    "    test_data_without_labels = test_data_sorted.drop(features_to_remove, axis=1)\n",
    "    test_data_with_labels = test_data_sorted\n",
    "    \n",
    "    return test_data_with_labels, test_data_without_labels, train\n",
    "    \n",
    "def sample_ids(dat, n): # test_prop is proportionate to the number of unique srch_id's, not number of instances\n",
    "\n",
    "    # unique_ids for searches that have more than 5 instances\n",
    "    grouped = dat.groupby('srch_id').count()\n",
    "    more_than_5_instances = grouped[grouped['site_id'] > 4]\n",
    "    \n",
    "    unique_ids = list(more_than_5_instances.index)\n",
    "    test_ids = random.sample(unique_ids,n)\n",
    "    \n",
    "    return test_ids\n",
    "\n",
    "def get_train_test(dat, n):\n",
    "    \n",
    "    test_ids = sample_ids(dat, n) # get ids for test set\n",
    "    test_ids_df = pd.DataFrame({'srch_id':test_ids}) # convert to df for .merge function\n",
    "    test = test_ids_df.merge(dat, on = 'srch_id', how = 'left')\n",
    "    \n",
    "    outer = dat.merge(test_ids_df, on = 'srch_id', how = 'outer', indicator = True)\n",
    "    train = outer[(outer._merge=='left_only')].drop('_merge', axis=1)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761625f8",
   "metadata": {},
   "source": [
    "## Experiment section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380241c",
   "metadata": {},
   "source": [
    "#### load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dat = pd.read_csv('/Users/kuba/VU/DMT/data-mining-techniques/A2/100k_train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa3c59",
   "metadata": {},
   "source": [
    "#### split data and save it (if you want to save it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_labels, test_data_without_labels, train = train_test_split(orginal_data)\n",
    "# save locally\n",
    "test_data_without_labels.to_csv('split_test_inference_data_100k.csv')\n",
    "test_data_with_labels.to_csv('split_test_data_100k.csv')\n",
    "train.to_csv('split_train_data_100k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189e9b1",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "df55599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('split_train_data_100k.csv')\n",
    "test_data_with_labels = pd.read_csv('split_test_data_100k.csv')\n",
    "test_data_inference = pd.read_csv('split_test_inference_data_100k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893a7cd",
   "metadata": {},
   "source": [
    "#### run cf experiment with evaluation\n",
    "CF is being run for different n values\n",
    "NDCG@5 scores are printed as an output\n",
    "\n",
    "\n",
    "if you want to play with CF settings focus on modyfing scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4c30c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3595469173306926\n",
      "0.4561681861902457\n",
      "0.4561531091801045\n",
      "0.4495358276015838\n",
      "0.45918203671908536\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# to turn off numpy warnings\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "cf = CollaborativeFiltering()\n",
    "# to check what is the best n for nearest searches\n",
    "for i in range(5):\n",
    "    out = cf.fit(train_data, test_data_inference, i)\n",
    "    print(evaluate(out, test_data_with_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9c7dd24c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/kuba/VU/DMT/2/data-mining-techniques/A2/100k_train_data_adjusted.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m test_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/kuba/VU/DMT/2/data-mining-techniques/A2/test_data_processed_adjusted.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m t_d \u001b[38;5;241m=\u001b[39m train_data\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# create a label for train data\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "train_data = pd.read_csv(\"/Users/kuba/VU/DMT/2/data-mining-techniques/A2/100k_train_data_adjusted.csv\")\n",
    "test_data = pd.read_csv(\"/Users/kuba/VU/DMT/2/data-mining-techniques/A2/test_data_processed_adjusted.csv\")\n",
    "\n",
    "t_d = train_data\n",
    "# create a label for train data\n",
    "t_d['relevance_score'] = 5*t_d[\"booking_bool\"] + t_d[\"click_bool\"] - t_d[\"position\"] - t_d['gross_bookings_usd']\n",
    "t_d.drop(['click_bool', 'gross_bookings_usd', 'booking_bool', 'position'], inplace=True, axis=1)\n",
    "label = pd.DataFrame({'relevance_score': t_d['relevance_score'].values})\n",
    "t_d.drop(['relevance_score'], inplace=True, axis=1)\n",
    "dtrain = xgb.DMatrix(t_d, label=label)\n",
    "\n",
    "# train model\n",
    "param = {'max_depth':6, 'eta':0.3, 'objective':'rank:pairwise'}\n",
    "model = xgb.train(param, dtrain, 50)\n",
    "\n",
    "# run inference\n",
    "test = test_data\n",
    "test = test.reindex(columns=t_d.columns)\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "dtest = xgb.DMatrix(test)\n",
    "scores = model.predict(dtest)\n",
    "scores = list(scores)\n",
    "output_list = []\n",
    "\n",
    "# sort and save data due to their relevance scores\n",
    "with open('relevance_scores_submission.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['srch_id', 'prop_id'])\n",
    "    order_list = []\n",
    "    prev = None\n",
    "    for idx, score in enumerate(scores):\n",
    "        hotel_id = test.iloc[idx]['prop_id']\n",
    "        srch_id = test.iloc[idx]['srch_id']\n",
    "        if(prev is not None and srch_id != prev):\n",
    "            order_list = sorted(order_list, reverse=True)\n",
    "            order_list = [[x[1], x[2]] for x in order_list]\n",
    "            # for el in order_list:\n",
    "            #    output_list.append(el)\n",
    "            writer.writerows(order_list)\n",
    "            order_list = []\n",
    "        prev = srch_id\n",
    "            \n",
    "        order_list.append([score, int(srch_id), int(hotel_id)])\n",
    "# print(output_list)            \n",
    "# print(evaluate(output_list, test_data_with_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00479036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e99515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

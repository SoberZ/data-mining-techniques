{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e875141",
   "metadata": {},
   "source": [
    "# Models, Evaluation, TrainTestSplit implementations\n",
    "\n",
    "#### if you wanna run experiment scroll down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3622df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import csv\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# user based collaborative filtering\n",
    "# if you want to run CF to create submission file uncomment write lines in fit function and comment output_list\n",
    "# lines so you dont store 2gb in your ram memory\n",
    "class CollaborativeFiltering:\n",
    "    \n",
    "    # we need more search features\n",
    "    # we can remove all hotel features\n",
    "    def __init__(self):\n",
    "        self.search_features = [\"srch_adults_count\", \"srch_saturday_night_bool\", \"site_id\", \"visitor_location_country_id\", \"srch_length_of_stay\", \"srch_room_count\"]\n",
    "    \n",
    "    def cos_sim(self, user_data, test_data):\n",
    "        categorical = [\"site_id\", \"visitor_location_country_id\"]\n",
    "        user_categorical, user_numeric = user_data[categorical] , user_data.drop(categorical)\n",
    "        test_categorical, test_numeric = test_data[categorical], test_data.drop(categorical)\n",
    "        \n",
    "        # for numeric calc cos_sim\n",
    "        a = user_numeric.to_numpy()\n",
    "        b = test_numeric.to_numpy()\n",
    "        \n",
    "        cos_sim_score = dot(a, b)/(norm(a)*norm(b))\n",
    "        \n",
    "        if(math.isnan(cos_sim_score)):\n",
    "            return -1\n",
    "        \n",
    "        # for cateforical_data increase cos_sim_score if it matches\n",
    "        for idx, col in enumerate(user_categorical):\n",
    "            if user_categorical[idx] == test_categorical[idx]:\n",
    "                cos_sim_score += 0.15\n",
    "    \n",
    "        return cos_sim_score\n",
    "    \n",
    "    def get_cosine_similarities(self, train_data, user_data):\n",
    "        cosine_scores = []\n",
    "        for idx, t_data in train_data.iterrows():\n",
    "            cosine_scores.append((self.cos_sim(user_data, t_data) , idx))\n",
    "            \n",
    "        return sorted(cosine_scores, reverse=True)\n",
    "    \n",
    "    def find_n_nearest_searches(self, user_data, n, hotel_id):\n",
    "        # take only those users who have data for this hotel\n",
    "        train_data = self.train_data.loc[self.train_data[\"prop_id\"] == hotel_id]\n",
    "        train_data = train_data[self.search_features]\n",
    "        # get n best rows (with highest cosine similiarity)\n",
    "        similiarities = self.get_cosine_similarities(train_data, user_data)[:n]\n",
    "        # print(f\"cosine sim {similiarities}\")\n",
    "        idx = [i[1] for i in similiarities]\n",
    "        return self.train_data.iloc[idx]\n",
    "        \n",
    "    def score_function(self, n_nearest_searches):\n",
    "        if n_nearest_searches.empty:\n",
    "            return -10\n",
    "        score = 0\n",
    "        b_w = 5 # booking weight\n",
    "        for _, x in n_nearest_searches.iterrows():\n",
    "            score += b_w * x[\"booking_bool\"] + x[\"click_bool\"] - x[\"position\"]\n",
    "        # return avg score \n",
    "        return score/len(n_nearest_searches)\n",
    "    \n",
    "    def fit(self, train_data, test_data, n=5):\n",
    "        self.train_data = train_data\n",
    "        output_list = []\n",
    "        # for every user search\n",
    "    # with open('cf.csv', 'w') as f:\n",
    "        # writer = csv.writer(f)\n",
    "        # writer.writerow([\"srch_id\", \"prop_id\"])\n",
    "        for srch_id in test_data[\"srch_id\"].unique():\n",
    "            user_searches = test_data.loc[test_data[\"srch_id\"] == srch_id]\n",
    "            hotel_scores = []\n",
    "            user_data = user_searches[self.search_features].iloc[0]\n",
    "            for idx, search in user_searches.iterrows():\n",
    "                # find n nearest users user_data + search_specific_data\n",
    "                hotel_id = search[\"prop_id\"]\n",
    "                # n nearest searches that scored a given hotel\n",
    "                n_nearest_searches = self.find_n_nearest_searches(user_data, n, hotel_id)\n",
    "\n",
    "                score = self.score_function(n_nearest_searches)\n",
    "                hotel_scores.append((score, hotel_id))\n",
    "\n",
    "            # ordered list of hotel_ids w.r.t. scores\n",
    "            hotel_scores = sorted(hotel_scores, reverse=True)\n",
    "            # only save hotel ids\n",
    "            hotel_scores = [[srch_id, int(h[1])] for h in hotel_scores]\n",
    "            # writer.writerows(hotel_scores)\n",
    "            output_list += hotel_scores\n",
    "\n",
    "        return output_list\n",
    "\n",
    "# train_data = pd.read_csv(\"/Users/kuba/VU/DMT/data-mining-techniques/A2/100k_train_data.csv\")\n",
    "# test_data = pd.read_csv(\"/Users/kuba/VU/DMT/data-mining-techniques/A2/processed_test_data.csv\")\n",
    "# cf = CollaborativeFiltering()\n",
    "# out = cf.fit(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "37625113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "# baseline random\n",
    "def random_ordering(test_data):\n",
    "    with open('sub_random.csv', 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"srch_id\", \"prop_id\"])\n",
    "        for srch_id in test_data[\"srch_id\"].unique():\n",
    "            search_list = []\n",
    "            user_searches = test_data.loc[test_data[\"srch_id\"] == srch_id]\n",
    "            for idx, search in user_searches.iterrows():\n",
    "                hotel_id = search[\"prop_id\"]\n",
    "                search_list.append([srch_id, int(hotel_id)])\n",
    "\n",
    "            random.shuffle(search_list)\n",
    "            writer.writerows(search_list)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "61c83ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def evaluate(predicted_ranking, actual_ranking, n=5):\n",
    "    \n",
    "    idcg = 5\n",
    "    for i in range(2, n + 1):\n",
    "        idcg += 1/math.log2(i + 1)\n",
    "        \n",
    "    i = 1\n",
    "    prev = predicted_ranking[0][0] # srch_id\n",
    "    all_ndcg = []\n",
    "    dcg = 0\n",
    "    \n",
    "    for row in predicted_ranking:\n",
    "    \n",
    "        if row[0] != prev:\n",
    "            i = 1\n",
    "            dcg = 0\n",
    "            \n",
    "        # take only first n values per search\n",
    "        if i < n + 1:\n",
    "            hotel_id = row[1]\n",
    "            # check relevance score based on if hotel was booked\n",
    "            relevance_score = 0\n",
    "            if actual_ranking[(actual_ranking['srch_id'] == row[0]) & (actual_ranking['prop_id'] == hotel_id)]['booking_bool'].any() == 1:\n",
    "                relevance_score = 5\n",
    "                # chceck relevance score based on if hotel was clicked\n",
    "            elif actual_ranking[(actual_ranking['srch_id'] == row[0]) & (actual_ranking['prop_id'] == hotel_id)]['click_bool'].any() == 1:\n",
    "                relevance_score = 1\n",
    "            # print(f\"{i}_{row[0]}_{hotel_id}: {relevance_score}\")\n",
    "            # calculate dcg iteratively per value\n",
    "            dcg += relevance_score/math.log2(i+1)\n",
    "\n",
    "        # if last partial score for dcg added then normalize and add to all ndcg list\n",
    "        if i == n:\n",
    "            # normalize\n",
    "            ndcg = dcg/idcg\n",
    "            all_ndcg.append(ndcg)\n",
    "            \n",
    "        prev = row[0]\n",
    "        i+= 1\n",
    "\n",
    "    return sum(all_ndcg)/len(all_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec1aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_test_split(dat, n=50):\n",
    "    random.seed(1)\n",
    "    train, test = get_train_test(dat)\n",
    "    features_to_remove = ['click_bool','position','booking_bool', 'gross_bookings_usd']\n",
    "    test_data_sorted = test.sort_values(['srch_id','booking_bool', 'click_bool','position'], ascending = [True, False, False, True])\n",
    "    test_data_sorted.reset_index(drop=True, inplace=True)\n",
    "    # correct_order just needed for debugging\n",
    "    # correct_order = test_data_sorted[['srch_id', 'prop_id']]\n",
    "    test_data_without_labels = test_data_sorted.drop(features_to_remove, axis=1)\n",
    "    test_data_with_labels = test_data_sorted\n",
    "    \n",
    "    return test_data_with_labels, test_data_without_labels, train\n",
    "    \n",
    "def sample_ids(dat, n): # test_prop is proportionate to the number of unique srch_id's, not number of instances\n",
    "\n",
    "    # unique_ids for searches that have more than 5 instances\n",
    "    grouped = dat.groupby('srch_id').count()\n",
    "    more_than_5_instances = grouped[grouped['site_id'] > 4]\n",
    "    \n",
    "    unique_ids = list(more_than_5_instances.index)\n",
    "    test_ids = random.sample(unique_ids,n)\n",
    "    \n",
    "    return test_ids\n",
    "\n",
    "def get_train_test(dat, n):\n",
    "    \n",
    "    test_ids = sample_ids(dat, n) # get ids for test set\n",
    "    test_ids_df = pd.DataFrame({'srch_id':test_ids}) # convert to df for .merge function\n",
    "    test = test_ids_df.merge(dat, on = 'srch_id', how = 'left')\n",
    "    \n",
    "    outer = dat.merge(test_ids_df, on = 'srch_id', how = 'outer', indicator = True)\n",
    "    train = outer[(outer._merge=='left_only')].drop('_merge', axis=1)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761625f8",
   "metadata": {},
   "source": [
    "## Experiment section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380241c",
   "metadata": {},
   "source": [
    "#### load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dat = pd.read_csv('/Users/kuba/VU/DMT/data-mining-techniques/A2/100k_train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa3c59",
   "metadata": {},
   "source": [
    "#### split data and save it (if you want to save it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ea0e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_labels, test_data_without_labels, train = train_test_split(orginal_data)\n",
    "# save locally\n",
    "test_data_without_labels.to_csv('split_test_inference_data_100k.csv')\n",
    "test_data_with_labels.to_csv('split_test_data_100k.csv')\n",
    "train.to_csv('split_train_data_100k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7189e9b1",
   "metadata": {},
   "source": [
    "#### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "df55599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('split_train_data_100k.csv')\n",
    "test_data_with_labels = pd.read_csv('split_test_data_100k.csv')\n",
    "test_data_inference = pd.read_csv('split_test_inference_data_100k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1893a7cd",
   "metadata": {},
   "source": [
    "#### run cf experiment with evaluation\n",
    "CF is being run for different n values\n",
    "NDCG@5 scores are printed as an output\n",
    "\n",
    "\n",
    "if you want to play with CF settings focus on modyfing scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b4c30c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3595469173306926\n",
      "0.4561681861902457\n",
      "0.4561531091801045\n",
      "0.4495358276015838\n",
      "0.45918203671908536\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# to turn off numpy warnings\n",
    "np.seterr(all='ignore')\n",
    "\n",
    "cf = CollaborativeFiltering()\n",
    "# to check what is the best n for nearest searches\n",
    "for i in range(5):\n",
    "    out = cf.fit(train_data, test_data_inference, i)\n",
    "    print(evaluate(out, test_data_with_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ab2cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

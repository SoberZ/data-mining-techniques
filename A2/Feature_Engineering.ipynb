{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "426ec4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import KNNImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import missingno as msno\n",
    "\n",
    "engineer_mode = True # True = Don't show plots etc.\n",
    "\n",
    "mode = \"train\" # Use train dataset\n",
    "\n",
    "if mode == \"train\":\n",
    "    samples = pd.read_csv(\"vu-dmt-assigment-2-2023/training_set_VU_DM.csv\")\n",
    "else:\n",
    "    samples = pd.read_csv(\"vu-dmt-assigment-2-2023/test_set_VU_DM.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c70b6467",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4958347 entries, 0 to 4958346\n",
      "Data columns (total 54 columns):\n",
      " #   Column                       Dtype  \n",
      "---  ------                       -----  \n",
      " 0   srch_id                      int64  \n",
      " 1   date_time                    object \n",
      " 2   site_id                      int64  \n",
      " 3   visitor_location_country_id  int64  \n",
      " 4   visitor_hist_starrating      float64\n",
      " 5   visitor_hist_adr_usd         float64\n",
      " 6   prop_country_id              int64  \n",
      " 7   prop_id                      int64  \n",
      " 8   prop_starrating              int64  \n",
      " 9   prop_review_score            float64\n",
      " 10  prop_brand_bool              int64  \n",
      " 11  prop_location_score1         float64\n",
      " 12  prop_location_score2         float64\n",
      " 13  prop_log_historical_price    float64\n",
      " 14  position                     int64  \n",
      " 15  price_usd                    float64\n",
      " 16  promotion_flag               int64  \n",
      " 17  srch_destination_id          int64  \n",
      " 18  srch_length_of_stay          int64  \n",
      " 19  srch_booking_window          int64  \n",
      " 20  srch_adults_count            int64  \n",
      " 21  srch_children_count          int64  \n",
      " 22  srch_room_count              int64  \n",
      " 23  srch_saturday_night_bool     int64  \n",
      " 24  srch_query_affinity_score    float64\n",
      " 25  orig_destination_distance    float64\n",
      " 26  random_bool                  int64  \n",
      " 27  comp1_rate                   float64\n",
      " 28  comp1_inv                    float64\n",
      " 29  comp1_rate_percent_diff      float64\n",
      " 30  comp2_rate                   float64\n",
      " 31  comp2_inv                    float64\n",
      " 32  comp2_rate_percent_diff      float64\n",
      " 33  comp3_rate                   float64\n",
      " 34  comp3_inv                    float64\n",
      " 35  comp3_rate_percent_diff      float64\n",
      " 36  comp4_rate                   float64\n",
      " 37  comp4_inv                    float64\n",
      " 38  comp4_rate_percent_diff      float64\n",
      " 39  comp5_rate                   float64\n",
      " 40  comp5_inv                    float64\n",
      " 41  comp5_rate_percent_diff      float64\n",
      " 42  comp6_rate                   float64\n",
      " 43  comp6_inv                    float64\n",
      " 44  comp6_rate_percent_diff      float64\n",
      " 45  comp7_rate                   float64\n",
      " 46  comp7_inv                    float64\n",
      " 47  comp7_rate_percent_diff      float64\n",
      " 48  comp8_rate                   float64\n",
      " 49  comp8_inv                    float64\n",
      " 50  comp8_rate_percent_diff      float64\n",
      " 51  click_bool                   int64  \n",
      " 52  gross_bookings_usd           float64\n",
      " 53  booking_bool                 int64  \n",
      "dtypes: float64(34), int64(19), object(1)\n",
      "memory usage: 2.0+ GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp6_rate_percent_diff</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>10404</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>21315</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>27348</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>29604</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "0        1  2013-04-04 08:32:15       12                          187   \n",
       "1        1  2013-04-04 08:32:15       12                          187   \n",
       "2        1  2013-04-04 08:32:15       12                          187   \n",
       "3        1  2013-04-04 08:32:15       12                          187   \n",
       "4        1  2013-04-04 08:32:15       12                          187   \n",
       "\n",
       "   visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  prop_id  \\\n",
       "0                      NaN                   NaN              219      893   \n",
       "1                      NaN                   NaN              219    10404   \n",
       "2                      NaN                   NaN              219    21315   \n",
       "3                      NaN                   NaN              219    27348   \n",
       "4                      NaN                   NaN              219    29604   \n",
       "\n",
       "   prop_starrating  prop_review_score  ...  comp6_rate_percent_diff  \\\n",
       "0                3                3.5  ...                      NaN   \n",
       "1                4                4.0  ...                      NaN   \n",
       "2                3                4.5  ...                      NaN   \n",
       "3                2                4.0  ...                      NaN   \n",
       "4                4                3.5  ...                      NaN   \n",
       "\n",
       "   comp7_rate  comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
       "0         NaN        NaN                      NaN         0.0        0.0   \n",
       "1         NaN        NaN                      NaN         0.0        0.0   \n",
       "2         NaN        NaN                      NaN         0.0        0.0   \n",
       "3         NaN        NaN                      NaN        -1.0        0.0   \n",
       "4         NaN        NaN                      NaN         0.0        0.0   \n",
       "\n",
       "   comp8_rate_percent_diff  click_bool  gross_bookings_usd  booking_bool  \n",
       "0                      NaN           0                 NaN             0  \n",
       "1                      NaN           0                 NaN             0  \n",
       "2                      NaN           0                 NaN             0  \n",
       "3                      5.0           0                 NaN             0  \n",
       "4                      NaN           0                 NaN             0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.info()\n",
    "samples.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85967a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "srch_id                              0\n",
       "date_time                            0\n",
       "site_id                              0\n",
       "visitor_location_country_id          0\n",
       "visitor_hist_starrating        4706481\n",
       "visitor_hist_adr_usd           4705359\n",
       "prop_country_id                      0\n",
       "prop_id                              0\n",
       "prop_starrating                      0\n",
       "prop_review_score                 7364\n",
       "prop_brand_bool                      0\n",
       "prop_location_score1                 0\n",
       "prop_location_score2           1090348\n",
       "prop_log_historical_price            0\n",
       "position                             0\n",
       "price_usd                            0\n",
       "promotion_flag                       0\n",
       "srch_destination_id                  0\n",
       "srch_length_of_stay                  0\n",
       "srch_booking_window                  0\n",
       "srch_adults_count                    0\n",
       "srch_children_count                  0\n",
       "srch_room_count                      0\n",
       "srch_saturday_night_bool             0\n",
       "srch_query_affinity_score      4640941\n",
       "orig_destination_distance      1607782\n",
       "random_bool                          0\n",
       "comp1_rate                     4838417\n",
       "comp1_inv                      4828788\n",
       "comp1_rate_percent_diff        4863908\n",
       "comp2_rate                     2933675\n",
       "comp2_inv                      2828078\n",
       "comp2_rate_percent_diff        4402109\n",
       "comp3_rate                     3424059\n",
       "comp3_inv                      3307357\n",
       "comp3_rate_percent_diff        4485550\n",
       "comp4_rate                     4650969\n",
       "comp4_inv                      4614684\n",
       "comp4_rate_percent_diff        4827261\n",
       "comp5_rate                     2735974\n",
       "comp5_inv                      2598327\n",
       "comp5_rate_percent_diff        4117248\n",
       "comp6_rate                     4718190\n",
       "comp6_inv                      4697371\n",
       "comp6_rate_percent_diff        4862173\n",
       "comp7_rate                     4642999\n",
       "comp7_inv                      4601925\n",
       "comp7_rate_percent_diff        4819832\n",
       "comp8_rate                     3041693\n",
       "comp8_inv                      2970844\n",
       "comp8_rate_percent_diff        4343617\n",
       "click_bool                           0\n",
       "gross_bookings_usd             4819957\n",
       "booking_bool                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c83ab37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"date_time\"] = pd.to_datetime(samples[\"date_time\"])\n",
    "samples['date_time'] = [datetime.timestamp(x) for x in samples[\"date_time\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6900da6b",
   "metadata": {},
   "source": [
    "Training samples and the test samples have almost the same amount of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe435b34",
   "metadata": {},
   "source": [
    "# Undersample our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1193769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the counts\n",
    "def plot_data(df, is_float=False):\n",
    "    num_rows = len(df.columns) // 2 + len(df.columns) % 2\n",
    "\n",
    "    fig, axs = plt.subplots(num_rows, 2, figsize=(10, num_rows*3))\n",
    "\n",
    "    for i, col in enumerate(df.columns):\n",
    "        ax = axs.flatten()[i]\n",
    "        if not is_float:\n",
    "            sns.histplot(df[col], ax=ax)\n",
    "        else:\n",
    "            sns.scatterplot(x=df.index, y=df[col], ax=ax)\n",
    "        ax.set_title(col)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    if len(df.columns) % 2 != 0:\n",
    "        fig.delaxes(axs.flatten()[-1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7567a3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Undersample_data\n",
    "def undersample_data(df, column, alpha):\n",
    "    X_train, y_train = df.drop(column, axis=1), df[column]\n",
    "    smote = RandomUnderSampler(random_state=27, sampling_strategy=alpha)\n",
    "    X, y = smote.fit_resample(X_train, y_train)\n",
    "    print(\"Before undersampling: \", Counter(y_train))\n",
    "    print(\"After undersampling: \", Counter(y))\n",
    "    oversampled_df = pd.concat([pd.DataFrame(X), pd.DataFrame(y)], axis=1)\n",
    "    return oversampled_df\n",
    "\n",
    "# Undersample multi data\n",
    "def undersample_multi_data(df, column, alpha=4):\n",
    "    X_train, y_train = df.drop(column, axis=1), df[column]\n",
    "    a, b = Counter(df[column]).most_common()[0]\n",
    "\n",
    "    # Create a dictionary with the desired sample count for class 4\n",
    "    sampling_strategy = {}\n",
    "    sampling_strategy[a] = round(b/alpha)\n",
    "\n",
    "    rus = RandomUnderSampler(random_state=27, sampling_strategy=sampling_strategy)\n",
    "    X, y = rus.fit_resample(X_train, y_train)\n",
    "    print(\"Before undersampling: \", Counter(y_train))\n",
    "    print(\"After undersampling: \", Counter(y))\n",
    "\n",
    "    oversampled_df = pd.concat([pd.DataFrame(X, columns=X_train.columns), pd.DataFrame(y, columns=[column])], axis=1)\n",
    "    return oversampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e649414",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64924805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_columns(df):\n",
    "    new_df = df\n",
    "\n",
    "    # Some variables we can set to -1\n",
    "    set_zero = [\"prop_review_score\", \"srch_query_affinity_score\", \"prop_location_score2\"]\n",
    "    new_df[set_zero] = new_df[set_zero].fillna(-1)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        new_df[\"gross_bookings_usd\"] = new_df[\"gross_bookings_usd\"].fillna(0)\n",
    "\n",
    "    # Fill missing competitor data with zeros\n",
    "    cols = [c for c in new_df.columns if \"comp\" in c]\n",
    "    new_df[cols] = new_df[cols].fillna(0)\n",
    "\n",
    "    to_fill = [\"visitor_hist_adr_usd\", \"visitor_hist_starrating\"]\n",
    "\n",
    "    # Calculate mean distances for each combination of visitor and property\n",
    "    mean_distances = new_df.groupby(['visitor_location_country_id', 'prop_id'])['orig_destination_distance'].mean().fillna(-1).to_dict()\n",
    "\n",
    "    # Replace NaN distances with corresponding means\n",
    "    new_df['orig_destination_distance'] = new_df.apply(\n",
    "        lambda row: mean_distances[(row['visitor_location_country_id'], row['prop_id'])]\n",
    "                    if pd.isnull(row['orig_destination_distance'])\n",
    "                    else row['orig_destination_distance'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Fill the remaining visitor_hist_adr_usd missing for a date_time, it will be filled with the last occurred value prior to that date_time\n",
    "    new_df[\"visitor_hist_adr_usd\"] = new_df[\"visitor_hist_adr_usd\"].fillna(-1)\n",
    "    new_df[\"visitor_hist_starrating\"] = new_df[\"visitor_hist_starrating\"].fillna(-1)\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2b2a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_sample_negative_values(df, col_name, fr=0.5):\n",
    "    # Assume df is your DataFrame and 'column_name' is the name of the column you want to undersample\n",
    "    minus_one_rows = df[df[col_name] == -1]\n",
    "    other_rows = df[df[col_name] != -1]\n",
    "\n",
    "    # Undersample the -1 rows: let's say we want to keep half of them\n",
    "    undersampled_minus_one_rows = minus_one_rows.sample(frac=fr, random_state=42)\n",
    "\n",
    "    # Concatenate the undersampled -1 rows with the other rows\n",
    "    undersampled_df = pd.concat([undersampled_minus_one_rows, other_rows])\n",
    "    \n",
    "    print(\"Before: \", len(minus_one_rows))\n",
    "    print(\"After: \", len(undersampled_minus_one_rows))\n",
    "    return undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e39ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner_samples = fill_columns(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3f5f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner_samples = cleaner_samples.drop(\"visitor_hist_starrating\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0feb9bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before undersampling:  Counter({0: 4819957, 1: 138390})\n",
      "After undersampling:  Counter({0: 197700, 1: 138390})\n"
     ]
    }
   ],
   "source": [
    "if mode == \"train\":\n",
    "    # Downsample the booking_bool column\n",
    "    undersampled = undersample_data(cleaner_samples, \"booking_bool\", 0.7)\n",
    "\n",
    "    # Multi downsampling\n",
    "#     to_under_sample = [(\"site_id\", 2), (\"prop_country_id\", 2), (\"srch_room_count\", 2)]\n",
    "#     end_df = undersampled\n",
    "#     for under, alpha in to_under_sample:\n",
    "#         end_df = undersample_multi_data(end_df, under, alpha)\n",
    "\n",
    "# # Downsample negative values\n",
    "# abc = [(\"visitor_hist_starrating\", 0.01), (\"visitor_hist_adr_usd\", 0.1)]\n",
    "# new_df = cleaner_samples.copy()\n",
    "# for a,b in abc:\n",
    "#     new_df = down_sample_negative_values(new_df, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f44f30a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if mode == \"train\" and not engineer_mode:\n",
    "    dff=end_df\n",
    "    transfer = [\"prop_review_score\"]\n",
    "    float_cols = dff.select_dtypes(include=[\"float64\"]).drop(transfer, axis=1).columns\n",
    "    cat_cols = pd.concat([dff.select_dtypes(include=[\"int64\"]), dff[transfer]]).columns\n",
    "    plot_data(dff[cat_cols])\n",
    "    plot_data(dff[float_cols], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac312e34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if mode == \"train\" and not engineer_mode:\n",
    "    def plot_correlation(df):\n",
    "        plt.figure(figsize = (24, 12))\n",
    "        corr = df.corr()\n",
    "        sns.heatmap(corr, annot = True, linewidths = 1)\n",
    "        plt.show()\n",
    "        return corr['booking_bool'].abs().sort_values(ascending = False)\n",
    "\n",
    "    res = plot_correlation(cleaner_samples)\n",
    "    res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f92ab",
   "metadata": {},
   "source": [
    "WORKS KINDA:\n",
    "    \n",
    "    - prop_location_score1, prop_location_score2: CORRELATION - prop_score\n",
    "    - gross_bookings_usd, srch_length_of_stay: CORRELATION - booking_stay\n",
    "\n",
    "NOT WORKING:\n",
    "\n",
    "    - prop_starrating, prop_review_score: LIGHT CORRELATION\n",
    "    - prop_country_id, visitor_location_country_id: CORRELATION\n",
    "    - visitor_hist_adr_usd, visitor_hist_starrating: BIG CORRELATION, DOESNT WORK\n",
    "    - srch_room_count, srch_adults_count: CORRELATION, intuitively not really a predictor or booking_bool\n",
    "    - srch_saturday_night_bool, srch_length_of_stay: INVERSE CORRELATION, searching doesnt predict booking per se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd676b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_new_features(df):\n",
    "    # Assumption: prop_log_historical_price is per night\n",
    "    # Difference between user's historical prices and the current hotel\n",
    "    df[\"user_hotel_price\"] = df[\"visitor_hist_adr_usd\"]-np.exp(df[\"prop_log_historical_price\"])\n",
    "    \n",
    "    # Total property desirability score\n",
    "    df[\"desirability\"] = df[\"prop_location_score2\"]+df[\"prop_location_score1\"]+df[\"prop_review_score\"]\n",
    "\n",
    "    # Desirability to price per night ratio\n",
    "    # High desirability and low total price is perfect\n",
    "    df[\"d_p_ratio\"] = df[\"desirability\"] / df[\"price_usd\"]\n",
    "    \n",
    "    # People per room\n",
    "    df[\"people_per_room\"] = (df[\"srch_adults_count\"]+df[\"srch_children_count\"])/df[\"srch_room_count\"]\n",
    "\n",
    "    # Convert to datetime data type\n",
    "#     df[\"date_time\"] = [datetime.fromtimestamp(x) for x in df[\"date_time\"]]\n",
    "#     df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "#     df['hour_id'] = df['date_time'].dt.hour # Extract day\n",
    "#     df['day_id'] = df['date_time'].dt.day # Extract day    \n",
    "#     df['season_id'] = df['date_time'].dt.quarter # Extract season\n",
    "    \n",
    "#     comp_col = [col for col in df.columns if col.endswith('rate')]\n",
    "#     dat_comp = df[comp_col]\n",
    "#     dat_comp_cheap = dat_comp[(dat_comp<0)].dropna(axis = 0, how = 'all').index\n",
    "#     dat_comp_exp = dat_comp[(dat_comp>0)].dropna(axis = 0, how = 'all').index\n",
    "#     dat_comp_same = dat_comp[(dat_comp==0)].dropna(axis = 0, how = 'all').index\n",
    "    \n",
    "#     dat_comp['comp_dat'] = 0\n",
    "#     dat_comp.loc[dat_comp_cheap,'comp_dat'] = 1\n",
    "#     dat_comp.loc[dat_comp_same,'comp_dat'] = 0\n",
    "#     dat_comp.loc[dat_comp_exp,'comp_dat'] = 1\n",
    "#     dat_comp.loc[dat_comp_cheap,'comp_dat'] = -1\n",
    "#     df['comp_rate_all'] = dat_comp['comp_dat']\n",
    "    \n",
    "    # Overall competitor score\n",
    "    # Total difference of the advantage according to the price\n",
    "    \n",
    "    df[\"overall_advantage\"] = 0\n",
    "    for i in range(1, 9):\n",
    "        rat = \"comp\"+str(i)+\"_rate\"\n",
    "        inv = \"comp\"+str(i)+\"_inv\"\n",
    "        dff = \"comp\"+str(i)+\"_rate_percent_diff\"\n",
    "        df[\"overall_advantage\"] += df[rat]*(df[dff]/100)*df[inv]*df[\"price_usd\"] \n",
    "    \n",
    "    # Remove all the comp cols\n",
    "    rates = [c for c in df.columns if c.endswith(\"rate\")]\n",
    "    invs = [c for c in df.columns if c.endswith(\"inv\")]\n",
    "    diffs = [c for c in df.columns if c.endswith(\"diff\")]\n",
    "\n",
    "    df = df.drop([\"date_time\", \"srch_adults_count\", \"srch_children_count\", \"srch_room_count\"], axis=1)\n",
    "    df = df.drop([\"prop_location_score2\", \"prop_location_score1\", \"prop_review_score\"] , axis=1)\n",
    "    df = df.drop([\"visitor_hist_adr_usd\", \"prop_log_historical_price\"], axis=1)\n",
    "    df = df.drop(rates+invs+diffs, axis=1)\n",
    "    return df\n",
    "\n",
    "clean_training2 = create_df_new_features(undersampled if mode == \"train\" else cleaner_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "231897ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000e+00, 7.0000e+00, 2.0000e+00, 1.5000e+01, 6.0000e+00,\n",
       "       4.0000e+00, 8.0000e+00, 3.1000e+01, 3.0000e+00, 2.4000e+01,\n",
       "       1.6000e+01, 1.4000e+01, 5.0000e+00, 1.0000e+01, 1.3000e+01,\n",
       "       3.5000e+01, 1.1000e+01, 1.7000e+01, 1.9000e+01, 9.0000e+00,\n",
       "       6.0790e+03, 2.0000e+01, 1.2000e+01, 5.9390e+03, 3.0000e+01,\n",
       "       6.1040e+03, 2.7000e+01, 2.2000e+01, 1.0576e+04, 7.7000e+01,\n",
       "       3.6000e+01, 1.8000e+01, 4.8310e+03, 1.0600e+02, 6.1350e+03,\n",
       "       2.3000e+01, 9.6000e+01, 3.4000e+01, 4.2000e+01, 6.5720e+03,\n",
       "       2.5000e+01, 4.3600e+03, 2.9000e+01, 3.2000e+01, 4.3540e+03,\n",
       "       5.5000e+01, 2.1000e+01, 2.8000e+01, 5.2580e+03, 4.1000e+01,\n",
       "       6.5740e+03, 3.8000e+01, 5.7000e+01, 3.4060e+03, 6.5530e+03,\n",
       "       4.4000e+01, 7.0340e+03, 6.2490e+03, 1.2600e+02, 3.3000e+01,\n",
       "       3.4550e+03, 6.1010e+03, 3.7000e+01, 7.4000e+01, 6.0000e+01,\n",
       "       6.1270e+03, 6.1030e+03, 7.5000e+01, 8.6000e+01, 2.6000e+01,\n",
       "       4.9000e+01, 9.3000e+01, 9.9000e+01, 6.1090e+03, 3.9000e+01,\n",
       "       3.2760e+03, 5.6000e+01, 9.4000e+01, 6.1110e+03, 3.4790e+03,\n",
       "       5.6130e+03, 5.0490e+03, 6.1060e+03, 1.6400e+02, 6.1000e+03,\n",
       "       4.5000e+01, 5.9200e+03, 1.3030e+03, 6.1020e+03, 5.4000e+01,\n",
       "       6.7260e+03, 1.0931e+04, 9.7420e+03, 1.1240e+03, 7.1000e+01,\n",
       "       6.0810e+03, 1.4200e+03, 5.0000e+01, 6.1340e+03, 9.0000e+01,\n",
       "       1.7200e+02, 6.4830e+03, 6.0190e+03, 6.3340e+03, 6.3700e+03,\n",
       "       6.0050e+03, 6.7240e+03, 1.0950e+03, 7.5570e+03, 6.6900e+03,\n",
       "       6.1790e+03, 6.5650e+03, 2.2470e+03, 3.0510e+03, 1.9100e+02,\n",
       "       5.4900e+03, 8.9000e+01, 6.0800e+03, 6.0740e+03, 4.4150e+03,\n",
       "       6.6000e+01, 4.3000e+01, 4.0000e+01, 1.9400e+02, 6.1290e+03,\n",
       "       5.8870e+03, 7.0820e+03, 5.9000e+01, 4.8000e+01, 5.2000e+01,\n",
       "       6.8050e+03, 6.1370e+03, 6.1990e+03, 6.1280e+03, 6.5380e+03,\n",
       "       2.0680e+03, 6.0610e+03, 6.1170e+03, 6.0730e+03, 7.0400e+03,\n",
       "       6.1180e+03, 3.3540e+03, 4.4130e+03, 8.1000e+01, 5.9860e+03,\n",
       "       6.1080e+03, 3.2480e+03, 5.7030e+03, 4.9720e+03, 4.0880e+03,\n",
       "       6.1360e+03, 6.5410e+03, 3.3680e+03, 6.3840e+03, 8.4000e+01,\n",
       "       6.5010e+03, 7.3530e+03, 6.8570e+03, 6.5120e+03, 6.2000e+01,\n",
       "       2.0830e+03, 5.7460e+03, 5.8170e+03, 1.0200e+02, 8.5000e+01,\n",
       "       7.4300e+03, 6.4760e+03, 6.8380e+03, 6.0240e+03, 1.0440e+03,\n",
       "       6.5350e+03, 1.2400e+02, 6.7000e+01, 5.1000e+01, 6.5370e+03,\n",
       "       6.6990e+03, 6.3000e+01, 2.0000e+02, 8.4910e+03, 6.5000e+01,\n",
       "       6.8490e+03, 6.1200e+03, 2.2240e+03, 6.6450e+03, 6.5430e+03,\n",
       "       6.4790e+03, 6.5040e+03, 4.4010e+03, 4.6000e+01, 6.0470e+03,\n",
       "       1.8130e+03, 6.5540e+03, 7.3990e+03, 1.0000e+02, 6.5110e+03,\n",
       "       7.2560e+03, 1.2300e+02, 6.4170e+03, 7.8000e+01, 6.9000e+01,\n",
       "       1.9800e+02, 7.3000e+01, 6.4800e+03, 1.5060e+03, 1.7620e+03,\n",
       "       2.7140e+03, 9.7000e+01, 4.7000e+01, 1.0100e+02, 6.1100e+03,\n",
       "       5.1630e+03, 3.2680e+03, 6.0420e+03, 6.8000e+01, 7.6500e+03,\n",
       "       7.4820e+03, 6.9510e+03, 1.1000e+02, 6.1190e+03, 3.3350e+03,\n",
       "       6.1070e+03, 1.0910e+03, 8.8000e+01, 1.6480e+03, 5.3000e+01,\n",
       "       6.2420e+03, 7.0000e+01, 5.3250e+03, 5.4520e+03, 4.3450e+03,\n",
       "       6.5020e+03, 3.2890e+03, 2.2870e+03, 1.1010e+03, 6.9030e+03,\n",
       "       8.7000e+01, 3.3530e+03, 1.0500e+02, 6.7230e+03, 2.5710e+03,\n",
       "       8.5540e+03, 6.5670e+03, 1.3450e+03, 5.3380e+03, 2.1950e+03,\n",
       "       6.1120e+03, 6.1000e+01, 6.7290e+03, 1.2627e+04, 3.2730e+03,\n",
       "       3.1250e+03, 2.8360e+03, 1.3200e+02, 5.5500e+03, 1.1100e+02,\n",
       "       6.1160e+03, 9.2000e+01, 2.1670e+03, 1.1155e+04, 6.6200e+03,\n",
       "       6.1050e+03, 9.3450e+03, 1.8100e+02, 1.1700e+02, 9.5000e+01,\n",
       "       1.4536e+04, 8.3000e+01, 4.3920e+03, 6.4910e+03, 1.7610e+03,\n",
       "       9.8000e+01, 8.0000e+01, 7.9980e+03, 6.1130e+03, 2.5920e+03,\n",
       "       6.8020e+03, 2.3250e+03, 4.9970e+03, 3.4930e+03, 5.8000e+01,\n",
       "       8.2000e+01, 7.1470e+03, 5.9790e+03, 7.7270e+03, 4.3650e+03,\n",
       "       6.5570e+03, 5.8060e+03, 1.8340e+03, 6.0320e+03, 6.2660e+03,\n",
       "       6.5130e+03, 3.0090e+03, 8.7930e+03, 1.1900e+02, 6.9540e+03,\n",
       "       2.6600e+02, 6.1300e+03, 3.4120e+03, 2.5310e+03, 1.3600e+02,\n",
       "       2.1310e+03, 2.4380e+03])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undersampled[\"comp1_rate_percent_diff\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff9abbf",
   "metadata": {},
   "source": [
    "From our analysis we can see that price_usd and srch_length_of_stay have some outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad357e",
   "metadata": {},
   "source": [
    "## Down- and upsampling the training dataset\n",
    "For some data points we need to downsample and for some we need to upsample the data. i.e. when we look at booking bool, that checks whether a hotel is booked, a lot of hotels have not been booked. This would introduce a big bias towards the non-booking side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d6f70ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_outliers_and_normalize(end_df):\n",
    "    # Filter out non-numeric columns\n",
    "    df_numeric = end_df\n",
    "\n",
    "    for column in df_numeric.select_dtypes(include=['float64']):\n",
    "        Q1 = df_numeric[column].quantile(0.25)\n",
    "        Q3 = df_numeric[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define a mask for values outside the IQR\n",
    "        mask = ((df_numeric[column] < (Q1 - 1.5 * IQR)) | (df_numeric[column] > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "        # Replace outliers with NaN\n",
    "        df_numeric.loc[mask, column] = np.nan\n",
    "\n",
    "    # Impute/Replace NaN values with means\n",
    "    df_numeric.fillna(df_numeric.mean(), inplace=True)\n",
    "\n",
    "    # Normalize all data\n",
    "    id_cols = [c for c in end_df if \"id\" in c]\n",
    "    non_id_df = df_numeric.drop(id_cols, axis=1) # remove the ids\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(non_id_df), columns=non_id_df.columns)\n",
    "    return pd.concat([df_scaled, end_df[id_cols]], axis=1) # add ids back\n",
    "\n",
    "res = find_outliers_and_normalize(clean_training2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c033f",
   "metadata": {},
   "source": [
    "# Adjust proportions of # of properties according to srch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "903f6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rows that add up as close as possible to the target_sum\n",
    "def select_rows(df, column, target_sum):\n",
    "    df = df.sample(frac=1).reset_index(drop=True)  # Randomly shuffle the rows\n",
    "    temp_sum = 0\n",
    "    selected_rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        if temp_sum + row[column] <= target_sum:\n",
    "            temp_sum += row[column]\n",
    "            selected_rows.append(index)\n",
    "        if temp_sum >= target_sum:\n",
    "            break\n",
    "    return df.loc[selected_rows]\n",
    "\n",
    "# cdf = pcad_df.groupby(\"prop_id\")[\"srch_id\"].nunique().reset_index()\n",
    "# cdf.columns = [\"prop_id\", 'counts']\n",
    "# smaller = cdf[cdf[\"counts\"]<5]\n",
    "# smaller_selected = select_rows(smaller, \"counts\", 1000)\n",
    "# smaller_selected.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "615e52b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smaller:  10214 proportion:  0.09999510499779725\n",
      "bigger:  91931 proportion:  0.9000048950022027\n"
     ]
    }
   ],
   "source": [
    "def adjust_proportions(pcad_df, col1, col2, col1_ratio=0.9, col2_ratio=0.1, total_records=100000):\n",
    "    count_df = pcad_df.groupby(col1)[col2].nunique().reset_index()\n",
    "    count_df.columns = [col1, 'count']\n",
    "    bigger = count_df[count_df[\"count\"]>=5]\n",
    "    smaller = count_df[count_df[\"count\"]<5]\n",
    "\n",
    "    # Calculate the desired total based on 'bigger' representing 90%\n",
    "    desired_total_len = len(bigger) /  0.9\n",
    "\n",
    "    # Calculate the desired number of 'smaller' samples to reach 10% of total\n",
    "    desired_smaller_len = int(desired_total_len  * 0.10)\n",
    "\n",
    "    # If 'smaller' represents more than 10% of the total, downsample it\n",
    "    if len(smaller) > desired_smaller_len:\n",
    "        smaller = smaller.sample(n=desired_smaller_len, random_state=42)\n",
    "\n",
    "    # Check new proportions\n",
    "    ls = len(smaller)\n",
    "    lb = len(bigger)\n",
    "    total_len = ls + lb\n",
    "\n",
    "    print(\"smaller: \", ls, \"proportion: \", ls/total_len)\n",
    "    print(\"bigger: \", lb, \"proportion: \", lb/total_len)\n",
    "    \n",
    "    # Get prop_ids in smaller and bigger\n",
    "    smaller_prop_ids = smaller[col1].values\n",
    "    bigger_prop_ids = bigger[col1].values\n",
    "\n",
    "    # Get rows in pcad_df that match prop_ids in smaller and bigger\n",
    "    smaller_rows = pcad_df[pcad_df[col1].isin(smaller_prop_ids)]\n",
    "    bigger_rows = pcad_df[pcad_df[col1].isin(bigger_prop_ids)]\n",
    "\n",
    "    # Combine back into a single dataframe\n",
    "    new_pcad_df = pd.concat([smaller_rows, bigger_rows], ignore_index=True)\n",
    "    return new_pcad_df\n",
    "\n",
    "# specify the columns you want to proportionate, and the ratios\n",
    "# Specify how many total_records you want\n",
    "adjusted = adjust_proportions(res, \"prop_id\", \"srch_id\", 0.9, 0.1, total_records=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e28d695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA if necessary\n",
    "def pca(df):\n",
    "    cols_to_scale = ['prop_starrating', \"price_per_night\", \"d_p_ratio\",\n",
    "                     'srch_length_of_stay', \"price_usd\", \"desirability\", \"user_hotel_price\", \n",
    "                     \"overall_advantage\", \"people_per_room\"]\n",
    "\n",
    "    df_to_pca, df_other = df[cols_to_scale], df.drop(cols_to_scale, axis=1)\n",
    "    \n",
    "    # Do PCA only on the cols_to_scale these are specified\n",
    "    pca = PCA()\n",
    "    dat = pca.fit_transform(df_to_pca)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    var_dat = pd.DataFrame({'variance' : explained_variance})\n",
    "    plt.bar(var_dat.index,var_dat['variance'])\n",
    "    plt.show()\n",
    "\n",
    "    pcas = var_dat.cumsum().sort_values(\"variance\", ascending=False)\n",
    "\n",
    "    cumulative_variance = var_dat.cumsum()\n",
    "    print(cumulative_variance)\n",
    "\n",
    "    # Find the number of components needed to capture 95% of the variance\n",
    "    n_components = len(cumulative_variance[cumulative_variance <= 0.95].dropna())\n",
    "\n",
    "    # Fit PCA again with the optimal number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    dat = pca.fit_transform(df_to_pca)\n",
    "    var_dat = pd.DataFrame({'variance' : explained_variance})\n",
    "    var_dat.cumsum()\n",
    "\n",
    "    pca_df = pd.concat([pd.DataFrame(dat), df_other], axis=1)\n",
    "    return pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb518cba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pcad_df = pca(adjusted)\n",
    "pcad_df = adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "357db962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4898753 entries, 0 to 4898752\n",
      "Data columns (total 21 columns):\n",
      " #   Column                       Dtype  \n",
      "---  ------                       -----  \n",
      " 0   prop_starrating              float64\n",
      " 1   prop_brand_bool              float64\n",
      " 2   price_usd                    float64\n",
      " 3   promotion_flag               float64\n",
      " 4   srch_length_of_stay          float64\n",
      " 5   srch_booking_window          float64\n",
      " 6   srch_saturday_night_bool     float64\n",
      " 7   srch_query_affinity_score    float64\n",
      " 8   orig_destination_distance    float64\n",
      " 9   random_bool                  float64\n",
      " 10  user_hotel_price             float64\n",
      " 11  desirability                 float64\n",
      " 12  d_p_ratio                    float64\n",
      " 13  people_per_room              float64\n",
      " 14  overall_advantage            float64\n",
      " 15  srch_id                      int64  \n",
      " 16  site_id                      int64  \n",
      " 17  visitor_location_country_id  int64  \n",
      " 18  prop_country_id              int64  \n",
      " 19  prop_id                      int64  \n",
      " 20  srch_destination_id          int64  \n",
      "dtypes: float64(15), int64(6)\n",
      "memory usage: 784.9 MB\n"
     ]
    }
   ],
   "source": [
    "pcad_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bacf2c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned data\n",
    "if mode == \"train\":\n",
    "    pcad_df.to_csv(\"100k_train_data.csv\")\n",
    "else:\n",
    "    pcad_df.sample(n=100000).to_csv(\"100k_test_data.csv\")\n",
    "\n",
    "# Get 10k of samples from the final df\n",
    "small_sample = pcad_df.sample(n=10000, random_state=1)\n",
    "small_sample.to_csv(\"10k_train.csv\" if mode == \"train\" else \"10k_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5bf7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8419391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e16ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
